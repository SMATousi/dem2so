{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1a49f76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import wandb\n",
    "from os.path import splitext\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import logging\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as TF\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import imageio.v2 as imageio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import wandb\n",
    "import random\n",
    "import numpy as np\n",
    "from model import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "149b7878",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ResNetFeatures(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        super(ResNetFeatures, self).__init__()\n",
    "        resnet = models.resnet50(pretrained=False)\n",
    "        resnet.fc = torch.nn.Linear(2048,19)\n",
    "#         resnet.conv1 = torch.nn.Conv2d(13, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        # Load your pretrained weights here if you have them\n",
    "        checkpoint = torch.load('../../models/PyTorch/B3_rn50_moco_0099_ckpt.pth')\n",
    "\n",
    "        # rename moco pre-trained keys\n",
    "        state_dict = checkpoint['state_dict']\n",
    "        #print(state_dict.keys())\n",
    "        for k in list(state_dict.keys()):\n",
    "            # retain only encoder up to before the embedding layer\n",
    "            if k.startswith('module.encoder_q') and not k.startswith('module.encoder_q.fc'):\n",
    "                #pdb.set_trace()\n",
    "                # remove prefix\n",
    "                state_dict[k[len(\"module.encoder_q.\"):]] = state_dict[k]\n",
    "            # delete renamed or unused k\n",
    "            del state_dict[k]\n",
    "        \n",
    "        '''\n",
    "        # remove prefix\n",
    "        state_dict = {k.replace(\"module.\", \"\"): v for k,v in state_dict.items()}\n",
    "        '''\n",
    "        #args.start_epoch = 0\n",
    "        resnet.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "        # Remove the fully connected layer and the average pooling layer\n",
    "        self.features = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(output_size)\n",
    "        \n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        return x\n",
    "\n",
    "class FusionNet(nn.Module):\n",
    "    def __init__(self, input_channels, output_size):\n",
    "        super(FusionNet, self).__init__()\n",
    "        self.conv = nn.Conv2d(input_channels, 1, kernel_size=1)  # Reduce to 1 channel\n",
    "        self.upsample = nn.Upsample(size=output_size, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.upsample(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RGB_DEM_to_SO(nn.Module):\n",
    "    def __init__(self, resnet_output_size, fusion_output_size):\n",
    "        super(RGB_DEM_to_SO, self).__init__()\n",
    "        self.resnet = ResNetFeatures(output_size=resnet_output_size)\n",
    "        self.fusion_net = FusionNet(input_channels=6*2048, output_size=fusion_output_size)\n",
    "        self.unet = UNet_1(n_channels=2, n_classes=8)\n",
    "\n",
    "    def forward(self, dem, rgbs):\n",
    "        # rgbs is a list of RGB images\n",
    "        features = [self.resnet(rgb) for rgb in rgbs]\n",
    "        features = torch.cat(features, dim=1)  # Concatenate features along the channel dimension\n",
    "        fused = self.fusion_net(features)\n",
    "\n",
    "        # Concatenate DEM and fused features\n",
    "        combined_input = torch.cat((dem, fused), dim=1)\n",
    "        so_output = self.unet(combined_input)\n",
    "\n",
    "        return so_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c9baedb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RGB_RasterTilesDataset(Dataset):\n",
    "    def __init__(self, dem_dir, so_dir, rgb_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Custom dataset to load DEM, SO, and RGB tiles.\n",
    "\n",
    "        :param dem_dir: Directory where DEM tiles are stored.\n",
    "        :param so_dir: Directory where SO tiles are stored.\n",
    "        :param rgb_dir: Directory where RGB tiles are stored.\n",
    "        :param transform: Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.dem_dir = dem_dir\n",
    "        self.so_dir = so_dir\n",
    "        self.rgb_dir = rgb_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        # self.filenames = [f for f in os.listdir(dem_dir) if os.path.isfile(os.path.join(dem_dir, f))]\n",
    "        self.tile_identifiers = [f.split('_')[2:4] for f in os.listdir(dem_dir) if 'dem_tile' in f]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tile_identifiers)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        tile_id = self.tile_identifiers[idx]\n",
    "        dem_file = os.path.join(self.dem_dir, f'dem_tile_{tile_id[0]}_{tile_id[1]}')\n",
    "        so_file = os.path.join(self.so_dir, f'so_tile_{tile_id[0]}_{tile_id[1]}')\n",
    "\n",
    "        # dem_file = os.path.join(self.dem_dir, self.filenames[idx])\n",
    "        # so_file = os.path.join(self.so_dir, self.filenames[idx])\n",
    "        # Assuming RGB tiles follow a similar naming convention\n",
    "        rgb_files = [os.path.join(self.rgb_dir, f'rgb{k}_tile_{tile_id[0]}_{tile_id[1]}') for k in range(6)]\n",
    "\n",
    "        dem_image = Image.open(dem_file)\n",
    "        so_image = Image.open(so_file)\n",
    "        rgb_images = [imageio.imread(file) for file in rgb_files]\n",
    "\n",
    "        dem_array = np.array(dem_image)\n",
    "        so_array = np.array(so_image)\n",
    "        rgb_arrays = [np.array(image).transpose(2,0,1)/255 for image in rgb_images]\n",
    "\n",
    "        sample = {'DEM': dem_array, 'SO': so_array, 'RGB': rgb_arrays}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "\n",
    "class RGB_RasterTransform:\n",
    "    \"\"\"\n",
    "    A custom transform class for raster data.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        dem, so, rgb = sample['DEM'], sample['SO'], sample['RGB']\n",
    "\n",
    "        # Random horizontal flipping\n",
    "        # if torch.rand(1) > 0.5:\n",
    "        #     dem = TF.hflip(dem)\n",
    "        #     so = TF.hflip(so)\n",
    "\n",
    "        # # Random vertical flipping\n",
    "        # if torch.rand(1) > 0.5:\n",
    "        #     dem = TF.vflip(dem)\n",
    "        #     so = TF.vflip(so)\n",
    "\n",
    "        # Convert numpy arrays to tensors\n",
    "        dem = TF.to_tensor(dem)\n",
    "        so = TF.to_tensor(so)\n",
    "        rgb_images = [TF.to_tensor(image) for image in rgb]\n",
    "\n",
    "        dem = TF.normalize(dem, 318.90567, 16.467052)\n",
    "\n",
    "        so = so.long()\n",
    "\n",
    "        return {'DEM': dem, 'SO': so.squeeze(), 'RGB': rgb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b2dd9347",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b9553607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "dem_dir = '/home/macula/SMATousi/Gullies/ground_truth/google_api/training_process/DEM2SO/dem2so/dem_with_rgb/dem'\n",
    "so_dir = '/home/macula/SMATousi/Gullies/ground_truth/google_api/training_process/DEM2SO/dem2so/dem_with_rgb/so'\n",
    "rgb_dir = '/home/macula/SMATousi/Gullies/ground_truth/google_api/training_process/DEM2SO/dem2so/dem_with_rgb/rgb'\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "learning_rate = 0.0001\n",
    "epochs = 10\n",
    "number_of_workers = 1\n",
    "image_size = 128\n",
    "val_percent = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a1dc4113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is loaded\n"
     ]
    }
   ],
   "source": [
    "transform = RGB_RasterTransform()\n",
    "\n",
    "dataset = RGB_RasterTilesDataset(dem_dir=dem_dir, so_dir=so_dir, rgb_dir=rgb_dir, transform=transform)\n",
    "\n",
    "# DataLoader\n",
    "\n",
    "n_val = int(len(dataset) * val_percent)\n",
    "n_train = len(dataset) - n_val\n",
    "train, val = random_split(dataset, [n_train, n_val])\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=number_of_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=number_of_workers, pin_memory=True, drop_last=True)\n",
    "\n",
    "print(\"Data is loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "325840cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RGB_DEM_to_SO(resnet_output_size=(8, 8), fusion_output_size=(128, 128)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cee2548e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|███████▉                                                  | 79/579 [00:51<05:26,  1.53it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_501148/262417938.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrgbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mso\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0miou\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmIOU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mso\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mtrain_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Train/iou'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0miou\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Gullies/ground_truth/google_api/training_process/DEM2SO/code/dem2so/utils.py\u001b[0m in \u001b[0;36mmIOU\u001b[0;34m(label, pred, num_classes)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mpred_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msem_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mtarget_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0msem_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtarget_inds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0miou_now\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nan'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "arg_nottest = True\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    train_metrics = {'Train/iou': 0}\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(train_loader)):\n",
    "        dem = batch['DEM'].to(device)\n",
    "        so = batch['SO'].to(device)\n",
    "        rgbs = [batch['RGB'][k].to(device) for k in range(6)]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(dem, rgbs)\n",
    "        loss = criterion(outputs, so)\n",
    "        iou = mIOU(so, outputs)\n",
    "        train_metrics['Train/iou'] += iou\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if arg_nottest:\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if arg_nottest:\n",
    "        for k in train_metrics:\n",
    "            train_metrics[k] /= len(train_loader)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] - Loss: {loss.item()}\")\n",
    "    print(train_metrics)\n",
    "    \n",
    "#         if (i+1) % 10 == 0:\n",
    "#             print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b0dca7d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/macula/SMATousi/.conda/envs/mac-deep/lib/python3.7/site-packages/ipykernel_launcher.py:37: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning dissapear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "dem = batch['DEM'].to(device)\n",
    "so = batch['SO'].to(device)\n",
    "rgbs = [batch['RGB'][k].to(device) for k in range(6)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "268245fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12288, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(dem, rgbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "00c8d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c6e61a64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13403529"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905e12d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
